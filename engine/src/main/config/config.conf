#Print streams in each spark iteration
printStreams = true

#Enable stats
statsEnabled = false
	
#Save all actions realized on the platform
auditEnabled = false
	
#Save all data into cassandra periodically
failover = {
	enabled = true
	#period = 300 s
	period = 20 s
}

#Cluster Identifier. Leave with default value if you don't need HA capabilities
clusterId = "node_1"

kafka = {
	hosts = ["localhost:9092"]
	connectionTimeout = 10000
	sessionTimeout = 10000
	
	# default replication factor and partitions for internal topics
	replicationFactor = 1
	partitions = 1

	# topics were this decision instance will be subscribed. Leave commented if you don't need Sharding capabilities
	dataTopics = ["topic_A"]
}
zookeeper = {
	hosts = ["localhost:2181"]
}
spark = {
	internalHost = "local[6]"
	internalStreamingBatchTime = 2 s
	
	host ="local[6]"
	streamingBatchTime = 2 s
}

cassandra = {
	hosts = ["localhost:9042"]
}

mongo = {
	hosts = ["localhost:27017"]
	#username = ""
	#password= ""
}

elasticsearch = {
	hosts = ["localhost:9300"]
	clusterName = "elasticsearch"
}

solr = {
	hosts = "localhost:2181"
	cloud = true
	dataDir = "/opt/sds/solr/examples/solr"
}

